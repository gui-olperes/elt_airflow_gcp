Este projeto implementa um pipeline de ELT (Extract, Load, Transform) utilizando o Apache Airflow (Astronomer), com armazenamento de dados em Google Cloud Storage (GCS), aplicando uma arquitetura em camadas: bronze, silver e gold.

ğŸ“‚ Estrutura do Projeto
makefile
Copiar
Editar
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ bronze_ingest_dag.py          # Sensor + transformaÃ§Ã£o bronze â†’ silver
â”‚   â”œâ”€â”€ silver_transform_dag.py       # Sensor + transformaÃ§Ã£o silver â†’ gold
â”‚   â””â”€â”€ gold_business_dag.py          # Regras de negÃ³cio finais (gold)
â”œâ”€â”€ include/
â”‚   â””â”€â”€ gcp_key.json                  # Chave de autenticaÃ§Ã£o (nÃ£o subir no Git!)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
ğŸ”— ConexÃ£o com GCP
A conexÃ£o com o Google Cloud Ã© feita via uma chave de serviÃ§o (gcp_key.json).

Gerar chave JSON em IAM > Service Accounts > Create Key.

Escapar a private_key com \\n e gerar o extra JSON no formato:

json
Copiar
Editar
{
  "keyfile_dict": {
    "type": "service_account",
    "...": "...",
    "private_key": "-----BEGIN PRIVATE KEY-----\\nABC...\\n-----END PRIVATE KEY-----\\n"
  },
  "scope": "https://www.googleapis.com/auth/cloud-platform"
}
Adicionar esse conteÃºdo no Airflow UI > Admin â†’ Connections > google_cloud_default.

ğŸ§± Camadas de Dados
Bronze: dados brutos recebidos (.csv)

Silver: dados limpos e transformados (.parquet)

Gold: dados prontos para anÃ¡lise com regras de negÃ³cio aplicadas

ğŸŒ€ Fluxo ELT com DAGs
ğŸ” 1. Bronze â†’ Silver
Sensor: aguarda qualquer .csv na pasta bronze/

Tarefa:

LÃª os arquivos .csv

Remove duplicatas e valores nulos

Adiciona coluna processado = True

Salva como .parquet em silver/

Move original para bronze/processed/

âœ¨ 2. Silver â†’ Gold
Sensor: aguarda silver/produtos.parquet

Tarefa:

LÃª o .parquet

Aplica regra de negÃ³cio (ex: agregaÃ§Ãµes)

Exporta como .parquet para gold/

ğŸ“¦ Operadores e Sensores usados
GCSHook: leitura e escrita em buckets

GCSObjectsWithPrefixExistenceSensor: aguarda arquivos em GCS

@dag e @task: estrutura moderna com Airflow 2.x+

Variable.get(): para parametrizar o nome do bucket

âœ… BenefÃ­cios do projeto
Automatiza o ciclo de ingestÃ£o, transformaÃ§Ã£o e entrega de dados.

Evita retrabalho com sensores e controle de arquivos processados.

Usa camadas bem definidas para organizaÃ§Ã£o e rastreabilidade.

Pode ser expandido facilmente para mÃºltiplas fontes e regras de negÃ³cio.
