# ğŸ› ï¸ ELT com Airflow + Google Cloud Storage

Este projeto implementa um pipeline de **ELT (Extract, Load, Transform)** utilizando o **Apache Airflow** (via Astronomer) e armazenamento de dados em **Google Cloud Storage (GCS)**. O pipeline segue uma arquitetura em camadas: **bronze â†’ silver â†’ gold**.

---

## ğŸ“‚ Estrutura do Projeto

```
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ transform_dag.py             # Bronze â†’ Silver
â”‚   â”œâ”€â”€ vendas_por_categoria_dag.py # Silver â†’ Gold
â”œâ”€â”€ include/
â”‚   â””â”€â”€ gcp_key.json                 # Chave GCP (nÃ£o subir ao Git!)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ”— ConexÃ£o com GCP

A conexÃ£o com o Google Cloud Ã© configurada por meio de uma chave de serviÃ§o (`gcp_key.json`).

1. Gere a chave JSON no painel do GCP (IAM â†’ Service Accounts).
2. Rode este script localmente para preparar o campo `extra` para a conexÃ£o no Airflow:

```python
import json

with open(r"C:\caminho\para\gcp_key.json") as f:
    keyfile_dict = json.load(f)

keyfile_dict["private_key"] = keyfile_dict["private_key"].replace("\n", "\\n")

extra_json = {
    "keyfile_dict": keyfile_dict,
    "scope": "https://www.googleapis.com/auth/cloud-platform"
}

print(json.dumps(extra_json, indent=2))
```

3. Copie o JSON impresso e cole na UI do Airflow em:  
   `Admin â†’ Connections â†’ google_cloud_default â†’ Extra`

---

## ğŸ§± Camadas de Dados

- **Bronze**: arquivos `.csv` brutos enviados ao GCS
- **Silver**: dados tratados (ex: remoÃ§Ã£o de duplicatas/nulos) em `.parquet`
- **Gold**: aplicaÃ§Ã£o de regras de negÃ³cio e geraÃ§Ã£o de datasets finais

---

## ğŸŒ€ Fluxo ELT com DAGs

### ğŸ” Bronze â†’ Silver (`transform_dag.py`)

- **Sensor**: espera por qualquer `.csv` na pasta `bronze/`
- **TransformaÃ§Ã£o**:
  - Remove duplicatas
  - Remove valores nulos
  - Adiciona coluna `processado = True`
- **SaÃ­da**:
  - Exporta `.parquet` para `silver/`
  - Move `.csv` original para `bronze/processed/`

### âœ¨ Silver â†’ Gold (`vendas_por_categoria_dag.py`)

- **Sensor**: espera pelo arquivo `silver/produtos.parquet`
- **TransformaÃ§Ã£o**:
  - Aplica lÃ³gica de negÃ³cio (ex: vendas por categoria)
  - Exporta resultado em `.parquet` para `gold/`

---

## ğŸ”§ Operadores e Sensores Utilizados

- `@dag`, `@task` â€” DAGs declarativas com Airflow moderno
- `GCSHook` â€” leitura e escrita em buckets GCS
- `GCSObjectsWithPrefixExistenceSensor` â€” sensor para aguardar arquivos
- `Variable.get()` â€” para parametrizaÃ§Ã£o do bucket via UI do Airflow

---

## âœ… Vantagens

- Evita reprocessamento com controle de arquivos
- Pipelines desacoplados e modulares por camada
- EscalÃ¡vel para mÃºltiplas fontes de dados
- Permite lÃ³gica de negÃ³cio na camada Gold
- IntegraÃ§Ã£o nativa com GCP

---

## ğŸš€ PrÃ³ximos Passos

- Versionamento de arquivos com timestamp no nome
- Upload dos dados finais para BigQuery
- Logs enriquecidos com metadados dos arquivos
- Monitoramento com alertas (Slack/Email)
- Deploy automatizado com CI/CD

---

> ğŸ’¡ **ObservaÃ§Ã£o**: NÃ£o versionar nem subir `gcp_key.json` em repositÃ³rios pÃºblicos.
